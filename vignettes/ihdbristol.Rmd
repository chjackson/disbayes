---
title: "Bayesian estimation of chronic disease epidemiology from incomplete data: the disbayes package"
author: Chris Jackson <chris.jackson@mrc-bsu.cam.ac.uk>
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{Bayesian estimation of chronic disease epidemiology from incomplete data: the disbayes package}
 %\VignetteEngine{knitr::rmarkdown}
 \usepackage[utf8]{inputenc}
bibliography: disbayes.bib
---


```{r}
knitr::opts_chunk$set(fig.width=7)
```


This document illustrates how case fatality can be estimated given incidence and mortality, using Bayesian modelling. 

This also serves to illustrate a general approach for estimating quantities describing disease epidemiology, based on indirect / incomplete data or multiple data sources.

The method is implemented in the R package `disbayes`, and could easily be adapted to novel situations.


# Theoretical disease model 

We represent a disease as a continuous-time, multi-state process with three states: 

1. disease-free
2. disease
3. dead from the disease.

If we assume that mortality from other causes is independent of disease status, deaths from other causes are uninformative and can be ignored.   Assume also for simplicity that remission from the disease is not possible (though the framework illustrated here is easily extensible to handle remission). 

The disease process is then fully defined by the 

* disease incidence $i(t)$, and the 

* case fatality rate $f(t)$,

which are both assumed to depend on age $t$.  Assume further that these rates are constant within integer years of age $a$, so they can be written $i_a$, $f_a$.

From these we can determine the _transition probability matrix_ $P_a$, whose $r,s$ entry $P_{ars}$ is the probability that a person is in state $s$ at age $a+1$ given they are in state $r$ at age $a$. The matrix $P_a$ is defined as a function of $i_a$ and $f_a$, which is the solution to a differential equation, and is written out explicitly in the DisMod II paper (@dismod2). 

Further, let $S_a$ be the "state occupancy probabilities", or the proportion of individuals in a hypothetical birth cohort (of infinite size) who are in each state at age $a$.  This is a row vector with three elements $S_{ar}$, one for each state $r$.  Assume everyone is disease-free at age 0.  The state occupancy probabilities at each subsequent age $a+1$ are then determined by mutiplying by the transition probability matrix: 

$$ S_{a+1} = S_a P_a $$ 

The prevalence of disease (among people who are alive) at each age $a$ is then obtained as $pr_a = S_{a2} / (S_{a1} + S_{a2})$. 

The disease-specific mortality rate at age $a$, or the probability that a person alive at age $a$ dies from the disease before age $a+1$, can also be expressed in terms of the disease prevalence at age $a$ and the transition probabilities between ages $a$ and $a+1$, as 

$$ dm_a = P_{a23} pr_a + P_{a13} (1 - pr_a) $$


# Bayesian approach to estimating the model from data

Data are observed which give information about some, but not all, of the parameters in the theoretical disease model.   The form of the data available may be different in each application.  We then wish to estimate any remaining unknown parameters.   The Bayesian approach to estimating these unknowns can be described as four steps:

1. write down a theoretical model for underlying disease progression (as done above)

2. write down a statistical model for the observed data given the parameters of the underlying disease model.    In this model, observed data need to be expressed as counts of individuals and associated denominators, e.g. 
   
  * (incidence): given a population of size $n_a^{(mort)}$, $r_a^{(inc)}$ of these are observed to get the disease within the next year 
   
  * (mortality): given a population of size $n_a^{(mort)}$ (with or without the disease), $r_a^{(mort)}$ of these are observed to die from the disease within the next year 
  
  * (prevalence): from a sample of $n_a^{(prev)}$ individuals, $r_a^{(prev)}$ are known to have the disease at age $a$ (and $n_a^{(prev)} - r_a^{(prev)}$ are known to not have the disease. 
  
The next section discusses how these data can be derived, given the typical forms of data available from burden of disease studies.   A denominator is required because it enables uncertainty associated with each estimate to be quantified, as we will discuss later. 

3. write down prior distributions for the unknowns.  These may express prior ignorance, as in the example below.

4. compute the (unique) posterior distribution of the unknown parameters in the joint model, given the observed data.

This approach is used in the DisMod-MR software, as explained by @flaxman2015, however the software itself is undocumented and not fully published.  The older (published) DisMod II (@dismod2) used an ad-hoc optimisation approach to estimate parameters. 

Advantages of the Bayesian method, implemented in `disbayes`, include 

* the ease of including multiple sources of direct/indirect data.  This is enabled by the computational methods and available software for Bayesian modelling, illustrated below.   This allows the approach to generalise to settings with different forms of data available.  In contrast, DisMod II only allows limited forms of data. 

* uncertainty about any quantity is quantified automatically through the posterior distribution, given the data, model assumptions and prior distributions supplied.

The use of general-purpose free software for this computation, through the `disbayes` package, is described below.



# Converting data into the required form for the Bayesian model

## ...Given estimates and denominators 

The following data are given in the R data frame `ihdbristol` supplied in the package, obtained from the Global Burden of Disease (REF) in a typical format:

* `inc` estimates of incidence of IHD by age

* `mort` estimates of IHD-specific mortality by age

* `pop` estimates of the population by age

and we wish to estimate case fatality by age, given these inputs.  A selection of rows from the full data frame are shown here.

```{r,show=FALSE}
library(disbayes)
ihdbristol[ihdbristol$age %in% 50:55, c("age","pop","inc","mort")]
```

Note that the published "data" `inc` and `mort` are themselves estimates, based on underlying data from a finite population. To use the Bayesian approach, we need to recreate the _numerator_ and _denominator_ underlying each of these estimates.   We assume that 

* the incidence has been estimated as $r_a^{(inc)}/n_a$, where out of $n_a$ people who are alive at age $a$, $r_a^{(inc)}$ of these get IHD before age $a+1$.

* the mortality has been estimated as $r_a^{(mort)}/n_a$, where out of $n_a$ people who are alive at age $a$, $r_a^{(mort)}$ of these die of IHD before age $a+1$.  These ratios are estimates of the true, underlying disease-specific mortalities $dm_a$. 

In this example, $n_a$ and $r_a$ were not provided in the original data source, but can be roughly recreated, as follows.  We were given population counts for five-year age groups, thus we obtain approximate one-year population counts $n_a$ by dividing these values by 5 (`pop` in the R data).

The number of incident IHD cases $r_a^{(inc)}$ can then be reconstructed by multiplying $n_a$ by the incidence rates `inc` supplied in the data.   Likewise, the number of IHD deaths $r_a$ (`ndieddis` in the R data) is then approximately reconstructed by multiplying $n_a$ by the IHD-specific mortality rates supplied in the spreadsheet.

The data also contain estimates of prevalence, implicitly based on observing the number of people with IHD $r^{(prev)}_a$ in a sample $n^{(prev)}_a$.    The assumed sample size $n^{(prev)}_a$ underlying these estimates is given in the variable `prevdenom`, which is multiplied by the prevalence estimates (given as `prev`) to obtain estimated prevalence counts $r^{(prev)}_a$ (given as `prevn`).   Typically, prevalence is estimated from survey data, though for diseases with comprehensive registers, such as cancer, the denominator underlying the estimate might be assumed to equal the population size. 

```{r,eval=TRUE}
ihdbristol[ihdbristol$age %in% 50:55, c("age","prev","prevn","prevdenom")]
```

## ...Given point and interval estimates

Sometimes a point estimate $\hat{p}$ for a quantity such as incidence is published alongside an (e.g. 95%) interval estimate $(\hat{p}^{(lower)},\hat{p}^{(upper)})$.   The interval estimate can be assumed to express the uncertainty associated with the estimate.   Such information can be converted to an implicit numerator $r$ and denominator $n$  as follows.   We assume the point and interval estimate are summaries of a Beta posterior distribution which has been obtained by combining a vague prior with an observation of $r$ events occurring out of a sample of $n$ individuals.   If a Beta(0,0) prior is used, then the posterior is known to be Beta(r, n-r).  We can then search for the best-fitting Beta(r,n-r)$ distribution which has median $\hat{p}$ and (2.5,97.5) quantiles $(\hat{p}^{(lower)},\hat{p}^{(upper)})$, and set $r=a, n=a+b$.   A utility to perform this search is provided in the SHELF package for expert elicitation (REF). 


## ...Expressing additional uncertainty

The uncertainty inherent in the information supplied about each of incidence, prevalence and mortality is measured by the denominator.  In the example above, this is the just size of the population used to produce the estimate.   However, if we also suspected that one of the data sources may be biased, but were unsure about the direction of bias, we could downweight that data source by multiplying both the numerator and denominator by the same amount, e.g. 0.5 if we wanted to give a data source half its original weight.  Note that if counts are scaled in this way, they should then be rounded to the nearest integer.


## Bayesian modelling process 

The four steps of the Bayesian modelling process are then implemented as follows:

1. write down the theoretical disease model, as given above

2. write down the statistical model for the data.  All count data are assumed to arise from a Binomial distribution with the corresponding denominator, and a probability parameter which is a function of the parameters in the theoretical disease model.

	* (incidence) $r_a^{(inc)} \sim ~ Binomial(n_a, P_{a12})$, where $P_{a12}$ is the annual transition probability from no disease to disease. 

	* (mortality)  $r_a^{(mort)} \sim ~ Binomial(n_a, dm_a)$, where the disease-specific mortality $dm_a$ is a deterministic function of the incidences and case fatalities $\{i_j,f_j\}$ for ages $j$ up to $a$, as described in the theoretical disease model. 

	* (prevalence)  $r_a^{(prev)} \sim ~ Binomial(n_a^{(prev)}, pr_a)$, where $pr_a$ is the true prevalence, defined as a deterministic function of the incidences and case fatalities.

3. define prior distributions for the unknown parameters.  For incidence, currently the `disbayes` package assumes an unbounded uniform prior for each age independently.  For case fatality, the package allows two alternatives, explained below.

4. compute the posterior distribution $p(\theta | \mathbf{y})$ for parameters $\theta = \{i_a,f_a\}$ given data $\mathbf{y} = \{i_a, r_a, n_a\}$



## Alternative models / prior assumptions 

In example such as this one, where quantities are estimated from indirect data, it is important to consider what substantive prior information is available about the unknown quantities, and what influence the assumed prior has on the resulting estimates. 

In these examples, the case fatality is only informed indirectly.  The `disbayes` package implements two alternative models for how case fatality depends on age.

1. the case fatality rate $f_a$ for each year of age $a$ is assumed to have an independent vague prior distribution, taken to be exponential(1).

2. the case fatality rate is assumed to be a smooth function $g()$ of age, $f_a = g(a)$.   This smooth function is defined by a spline basis $log(f_a) = \sum_k \beta_k g_k(a)$, where $g_k()$ are basis functions.  A "thin plate" spline is used, following the default in the `mgcv` package (Wood), and the amount of smoothness is determined automatically through a hierarchical prior on the coefficients (REF Wood, `jagam` paper). 

In addition, for all ages below a given cut-off age $a_{base}$, case fatalities are assumed to be equal to a constant $f_{base}$.   This cut-off age $a_{base}$ needs to be supplied when calling the `disbayes` function.  $f_{base}$ does not need to be supplied however, and is estimated from the data under the assumption that the dependence on age is a smooth function. 

When running `disbayes`, both models are fitted by default.  Model (2) is more realistic.  However it may not give sensible estimates if the information provided by the data is too weak. The results of fitting Model (1) can help to diagnose where the indirect information on case fatality provided by the rest of the data is weaker or stronger.   If the information on $f_a$ for a particular age is too weak, then the posterior distribution will be identical to the prior.   In those cases, substantive prior information about case fatality at that age is necessary for the estimates to be meaningful.

This information might come from nearby ages that are better-informed, through extrapolation from the smoothing model.  However extrapolating in this way is only feasible for a limited time span (perhaps around 10-20 years) before stronger assumptions are necessary, e.g. that case fatality is constant below a certain age.



# Fitting Bayesian models with the Stan software via disbayes

The Stan software ([mc-stan.org](http://mc-stan.org)) allows any Bayesian model to be written down in the Stan language.  The software then enables a sample from the posterior distribution to be drawn, using Hamiltonian Monte Carlo sampling.

The `disbayes` package uses Stan to fit the Bayesian disease model given here to the data $\mathbf{y} = \{i_a, r_a, n_a\}$ supplied in the `ihdbristol` data frame.   The function `disbayes` does the work of converting the data to the required format for Stan, and calling the Stan software to run the simulations.

The `disbayes` function requires the following arguments

 * `dat`: a data frame where all the variables are stored 

 * additional arguments indicating which columns of the data contain which variables.

The required variables include at least information about incidence and mortality, optionally information about prevalence, and some indication of uncertainty around this information.   This information can be supplied in three alternative ways.  For example, for incidence, 

1. estimate with denominator: through arguments `inc` and `inc_denom`.  The numerator is computed automatically, as described above. 

2. numerator and denominator, through arguments `inc_num` and `inc_denom`

3. estimate with lower and upper 95% credible limits, through arguments `inc`, `inc_lower` and `inc_upper`. The numerator and denominator are comptue computed automatically, as described above. 

The value given for the argument is a character string identifying the corresponding variable in the data.  For mortality and prevalence, arguments with names beginning `mort` and `prev` are supplied in the same manner. 

Before running the `disbayes` function to fit the models, we set up the computer for parallel processing.  This allows multiple processor cores to be used, which enables the models to fit substantially faster. 
```{r}
options(mc.cores = parallel::detectCores())
```

The following call to the `disbayes` function is then made to fit the model to the `ihdbristol` data.   The argument `eqage=40` defines an assumption that case fatality is constant for all ages below 40. 

```{r, eval=TRUE, cache=TRUE, warning=FALSE, message=FALSE}
dbres <- disbayes(dat = ihdbristol,
                 inc = "inc", 
                 inc_denom = "pop", 
                 prev_num = "prevn", 
                 prev_denom = "prevdenom",
                 mort = "mort",
                 mort_denom = "pop",
                 eqage = 40
                 )
```

The computation may take a few minutes, or less depending on the number of cores and their speed. 

The `disbayes` function returns a list with components `fit` and `fitu`, containing the smoothed and unsmoothed model fits respectively.  Each of these components is an object in the `stanfit` format defined by the `rstan` R interface to Stan


# IHD example: Results 

The results could be summarised and manipulated using functions provided by rstan, e.g. the `summary` and `extract` method for `stanfit` objects.    For example, we can check the MCMC simulations have converged, by examining the "trace plot" of the simulation progress for selected parameters. The simulated chains should mix together and look like white noise.

```{r,eval=TRUE}
rstan::traceplot(dbres$fit, pars=paste0("cf[", 60:65, "]"))
```

The `disbayes` package provides some shortcuts for extracting summary statistics.   The posterior medians and 95% credible intervals can be extracted from the fitted smooth model using the `summary` method for `disbayes` objects.   

The result is a data frame with the variables indicated in the row names.   These variables include `inc` (incidence), `cf` (case fatality), `mort` (mortality), `prev` (prevalence), each indexed by year of age.   The first few rows of the summary below contain estimates of incidence for the first few years of age. 

```{r, eval=TRUE}
summ <- summary(dbres)
head(summ)
```

To extract results for a specific variable, the `vars` argument can be provided, e.g. for case fatality: 

```{r,eval=TRUE}
summ <- summary(dbres, vars="cf")
head(summ)
```

To extract results from the unsmoothed model, the `summary_disbayes_fit` method is used.

```{r,eval=TRUE}
summ <- summary_disbayes_fit(dbres$fit, vars="cf")
summu <- summary_disbayes_fit(dbres$fitu, vars="cf")
head(summu)
```

The default `plot` method for objects returned by `disbayes` plots the posterior summaries for case fatality.  Both the smoothed and unsmoothed estimates from age 40 are shown.   This plot also overlays the corresponding estimates from DisMod II are overlaid in purple, included in the `ihdbristol` data in variable `dismod_cf`. 

```{r, eval=TRUE, warning=FALSE}
library(ggplot2)
plot(dbres) +  ylab("Case fatality") + xlab("Age") + ylim(0,0.5) + xlim(40,100) + 
  geom_line(aes(y=dismod_cf), data=ihdbristol, col="purple")
```

Under the unsmoothed model, the uncertainty about case fatality is biggest for the youngest and oldest ages, where either prevalence is lowest or the proportion of the cohort who are still alive is lowest. 

The smoothed model givese more precise estimates, aided by the assumption that case fatalities are similar between similar ages, and that case fatality is constant under the age of 40.  The smoothed model also removes the artefacts from the 5-year age grouping that are seen in the unsmoothed results. 

The smoothed model agrees with the DisMod II estimates, and has the benefit of characterising uncertainty as a posterior distribution.  Uncertainty about case fatality in this example arises from the disease prevalence.   If the prevalence is lower, fewer people have the disease, giving less information from which to estimate the case fatality for people with the disease. 




## Other things that could be done

* More ways of supplying substantive prior information 

* Account for variations between populations in different areas.  Hierarchical modelling might be used here - DISMOD-MR does this, I believe.

* Hierarchical modelling to account for variations between ordered socioeconomic categories / income groups

* Borrow information between men/women

* Jointly model multiple diseases.  i.e. relaxing the assumption that mortality from causes other than the disease is the same for people with and without the disease.  Multimorbidity. 

* Account for variations in incidence and prevalence with time as well as age.  DisMod II allows time trends to be modelled, but assumes that time trends are independent of age.  Relaxing this assumption would be feasible, but possibly computationally expensive. 
	* For IHD in the UK, data on time trends may come from the Health Survey for England, years 1993, 1994, 1988, 2003, 2006, 2011?

* In general, to transparently represent uncertainty, and explicitly build in other sources of information, in situations where we suspect the data may not be representative

All of these would be theoretically feasible in the above framework. The limits will be computational, and difficult to tell before getting hands on.


## References
